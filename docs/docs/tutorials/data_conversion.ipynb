{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bastionlab import Identity\n",
    "\n",
    "# Create `Identity` for Data owner.\n",
    "data_owner = Identity.create(\"data_owner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python datagen.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "!unzip smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identifier: \"bcda335b-eb73-4fc2-b4e1-c460975c5438\"\n",
      "meta: \"\\n\\005\\n\\003\\372\\001j\\n\\005\\n\\003\\372\\001j\\022\\005Int64\\022\\005Int64\\030\\372\\001\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "file_path = \"./SMSSpamCollection\"\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "with open(file_path) as f:\n",
    "    for line in f.readlines():\n",
    "        split = line.split(\"\\t\")\n",
    "        labels.append(1 if split[0] == \"spam\" else 0)\n",
    "        texts.append(split[1])\n",
    "df = pl.DataFrame({\"label\": labels, \"text\": texts})\n",
    "from bastionlab import Connection\n",
    "\n",
    "connection = Connection(\"localhost\")\n",
    "\n",
    "from bastionlab.polars.policy import Policy, Aggregation, Log\n",
    "\n",
    "policy = Policy(safe_zone=Aggregation(min_agg_size=10), unsafe_handling=Log())\n",
    "rdf = connection.client.polars.send_df(df.limit(500), policy=policy, sanitized_columns=[\"text\"])\n",
    "test_rdf = connection.client.polars.send_df(\n",
    "    df.tail(int(df.height / 3)).limit(250), policy=policy, sanitized_columns=[\"text\"]\n",
    ")\n",
    "import torch\n",
    "from typing import List, Optional, Union, Tuple\n",
    "\n",
    "\n",
    "train_rds = rdf.to_dataset([\"text\"], \"label\", \"distilbert-base-uncased\")\n",
    "test_rds = test_rdf.to_dataset([\"text\"], \"label\", \"distilbert-base-uncased\")\n",
    "\n",
    "train_rds._set_test_dataset(test_rds.train_dataset_ref)\n",
    "test_rds._update(train_dataset=train_rds.train_dataset_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from bastionlab.torch.utils import MultipleOutputWrapper\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    torchscript=True,\n",
    ")\n",
    "model = MultipleOutputWrapper(model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__list__ [ds] [(identifier: \"bcda335b-eb73-4fc2-b4e1-c460975c5438\"\n",
      "meta: \"\\n\\005\\n\\003\\372\\001j\\n\\005\\n\\003\\372\\001j\\022\\005Int64\\022\\005Int64\\030\\372\\001*G\\n$4974a0cf-c473-422b-8e51-027bbb5598b1\\\"\\037\\n\\005\\n\\003\\364\\003a\\n\\005\\n\\003\\364\\003a\\022\\005Int64\\022\\005Int64\\030\\364\\003\"\n",
      ", identifier: \"4974a0cf-c473-422b-8e51-027bbb5598b1\"\n",
      "meta: \"\\n\\005\\n\\003\\364\\003a\\n\\005\\n\\003\\364\\003a\\022\\005Int64\\022\\005Int64\\030\\364\\003\"\n",
      "), (identifier: \"01d962bb-9f69-4b5f-82f9-6857cea9f4a9\"\n",
      "meta: \"\\n\\005\\n\\003\\364\\003a\\n\\005\\n\\003\\364\\003a\\022\\005Int64\\022\\005Int64\\030\\364\\003\"\n",
      ", None), (identifier: \"80f41419-30c8-4019-9dff-cbda62bfde1a\"\n",
      "meta: \"\\n\\005\\n\\003\\372\\001j\\n\\005\\n\\003\\372\\001j\\022\\005Int64\\022\\005Int64\\030\\372\\001*G\\n$01d962bb-9f69-4b5f-82f9-6857cea9f4a9\\\"\\037\\n\\005\\n\\003\\364\\003a\\n\\005\\n\\003\\364\\003a\\022\\005Int64\\022\\005Int64\\030\\364\\003\"\n",
      ", identifier: \"01d962bb-9f69-4b5f-82f9-6857cea9f4a9\"\n",
      "meta: \"\\n\\005\\n\\003\\364\\003a\\n\\005\\n\\003\\364\\003a\\022\\005Int64\\022\\005Int64\\030\\364\\003\"\n",
      "), (identifier: \"4974a0cf-c473-422b-8e51-027bbb5598b1\"\n",
      "meta: \"\\n\\005\\n\\003\\364\\003a\\n\\005\\n\\003\\364\\003a\\022\\005Int64\\022\\005Int64\\030\\364\\003\"\n",
      ", None)]\n",
      "[<bastionlab.torch.learner.RemoteDataset object at 0x7fd2386efbb0>, <bastionlab.torch.learner.RemoteDataset object at 0x7fd2386ef1c0>, <bastionlab.torch.learner.RemoteDataset object at 0x7fd179fee130>, <bastionlab.torch.learner.RemoteDataset object at 0x7fd2386efb80>]\n",
      "[[tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])], [tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])], [tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])], [tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])]]\n"
     ]
    }
   ],
   "source": [
    "remote_datasets = connection.client.torch.list_remote_datasets()\n",
    "print(remote_datasets)\n",
    "\n",
    "print([r.trace_input for r in remote_datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (106) must match the size of tensor b (250) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotSupportedError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Projects/bastionlab/client/src/bastionlab/torch/learner.py:257\u001b[0m, in \u001b[0;36mRemoteLearner.__init__\u001b[0;34m(self, client, model, remote_dataset, loss, max_batch_size, optimizer, device, max_grad_norm, metric_eps_per_batch, model_name, model_description, expand, progress)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mscript(model)\n\u001b[1;32m    258\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_script.py:1286\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     obj \u001b[39m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1286\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49mcreate_script_module(\n\u001b[1;32m   1287\u001b[0m         obj, torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49minfer_methods_to_compile\n\u001b[1;32m   1288\u001b[0m     )\n\u001b[1;32m   1290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_recursive.py:458\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    457\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[39m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 458\u001b[0m \u001b[39mreturn\u001b[39;00m create_script_module_impl(nn_module, concrete_type, stubs_fn)\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_recursive.py:470\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    469\u001b[0m cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_create_module_with_type(concrete_type\u001b[39m.\u001b[39mjit_type)\n\u001b[0;32m--> 470\u001b[0m method_stubs \u001b[39m=\u001b[39m stubs_fn(nn_module)\n\u001b[1;32m    471\u001b[0m property_stubs \u001b[39m=\u001b[39m get_property_stubs(nn_module)\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_recursive.py:739\u001b[0m, in \u001b[0;36minfer_methods_to_compile\u001b[0;34m(nn_module)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[39mfor\u001b[39;00m method \u001b[39min\u001b[39;00m uniqued_methods:\n\u001b[0;32m--> 739\u001b[0m     stubs\u001b[39m.\u001b[39mappend(make_stub_from_method(nn_module, method))\n\u001b[1;32m    740\u001b[0m \u001b[39mreturn\u001b[39;00m overload_stubs \u001b[39m+\u001b[39m stubs\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_recursive.py:66\u001b[0m, in \u001b[0;36mmake_stub_from_method\u001b[0;34m(nn_module, method_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m# Make sure the name present in the resulting AST will match the name\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# requested here. The only time they don't match is if you do something\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m# like:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m# In this case, the actual function object will have the name `_forward`,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m# even though we requested a stub for `forward`.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[39mreturn\u001b[39;00m make_stub(func, method_name)\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_recursive.py:51\u001b[0m, in \u001b[0;36mmake_stub\u001b[0;34m(func, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m rcb \u001b[39m=\u001b[39m _jit_internal\u001b[39m.\u001b[39mcreateResolutionCallbackFromClosure(func)\n\u001b[0;32m---> 51\u001b[0m ast \u001b[39m=\u001b[39m get_jit_def(func, name, self_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mRecursiveScriptModule\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m ScriptMethodStub(rcb, ast, func)\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/frontend.py:265\u001b[0m, in \u001b[0;36mget_jit_def\u001b[0;34m(fn, def_name, self_name, is_classmethod)\u001b[0m\n\u001b[1;32m    263\u001b[0m     pdt_arg_types \u001b[39m=\u001b[39m type_trace_db\u001b[39m.\u001b[39mget_args_types(qualname)\n\u001b[0;32m--> 265\u001b[0m \u001b[39mreturn\u001b[39;00m build_def(parsed_def\u001b[39m.\u001b[39;49mctx, fn_def, type_line, def_name, self_name\u001b[39m=\u001b[39;49mself_name, pdt_arg_types\u001b[39m=\u001b[39;49mpdt_arg_types)\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/frontend.py:303\u001b[0m, in \u001b[0;36mbuild_def\u001b[0;34m(ctx, py_def, type_line, def_name, self_name, pdt_arg_types)\u001b[0m\n\u001b[1;32m    299\u001b[0m r \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mmake_range(py_def\u001b[39m.\u001b[39mlineno \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(py_def\u001b[39m.\u001b[39mdecorator_list),\n\u001b[1;32m    300\u001b[0m                    py_def\u001b[39m.\u001b[39mcol_offset,\n\u001b[1;32m    301\u001b[0m                    py_def\u001b[39m.\u001b[39mcol_offset \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdef\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 303\u001b[0m param_list \u001b[39m=\u001b[39m build_param_list(ctx, py_def\u001b[39m.\u001b[39;49margs, self_name, pdt_arg_types)\n\u001b[1;32m    304\u001b[0m return_type \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/frontend.py:327\u001b[0m, in \u001b[0;36mbuild_param_list\u001b[0;34m(ctx, py_args, self_name, pdt_arg_types)\u001b[0m\n\u001b[1;32m    326\u001b[0m     ctx_range \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mmake_range(expr\u001b[39m.\u001b[39mlineno, expr\u001b[39m.\u001b[39mcol_offset \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, expr\u001b[39m.\u001b[39mcol_offset \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(expr\u001b[39m.\u001b[39marg))\n\u001b[0;32m--> 327\u001b[0m     \u001b[39mraise\u001b[39;00m NotSupportedError(ctx_range, _vararg_kwarg_err)\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m py_args\u001b[39m.\u001b[39mvararg \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotSupportedError\u001b[0m: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:\n  File \"/home/kbamponsem/Projects/bastionlab/client/src/bastionlab/torch/utils.py\", line 392\n    def forward(self, *args, **kwargs) -> Tensor:\n                              ~~~~~~~ <--- HERE\n        output = self.inner.forward(*args, **kwargs)\n        return output[self.output]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbastionlab\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer_config\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m remote_learner \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mtorch\u001b[39m.\u001b[39;49mRemoteLearner(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     remote_datasets[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     max_batch_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     loss\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcross_entropy\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49mAdam(lr\u001b[39m=\u001b[39;49m\u001b[39m5e-5\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDistilBERT\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/bastionlab/client/src/bastionlab/torch/client.py:227\u001b[0m, in \u001b[0;36mBastionLabTorch.RemoteLearner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39m\"\"\"Returns a RemoteLearner object encapsulating a model and hyperparameters for\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39mtraining and testing on the remote server and that uses this client to communicate with the server.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m    **kwargs: all keyword arguments are forwarded to the `RemoteDataLoader` constructor.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlearner\u001b[39;00m \u001b[39mimport\u001b[39;00m RemoteLearner\n\u001b[0;32m--> 227\u001b[0m \u001b[39mreturn\u001b[39;00m RemoteLearner(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/bastionlab/client/src/bastionlab/torch/learner.py:259\u001b[0m, in \u001b[0;36mRemoteLearner.__init__\u001b[0;34m(self, client, model, remote_dataset, loss, max_batch_size, optimizer, device, max_grad_norm, metric_eps_per_batch, model_name, model_description, expand, progress)\u001b[0m\n\u001b[1;32m    257\u001b[0m         model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mscript(model)\n\u001b[1;32m    258\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> 259\u001b[0m         model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mtrace(  \u001b[39m# Compile the model with the tracing strategy\u001b[39;49;00m\n\u001b[1;32m    260\u001b[0m             \u001b[39m# Wrapp the model to use the first output only (and drop the others)\u001b[39;49;00m\n\u001b[1;32m    261\u001b[0m             model,\n\u001b[1;32m    262\u001b[0m             [x\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m remote_dataset\u001b[39m.\u001b[39;49mtrace_input],\n\u001b[1;32m    263\u001b[0m         )\n\u001b[1;32m    264\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_ref \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39msend_model(\n\u001b[1;32m    265\u001b[0m         model,\n\u001b[1;32m    266\u001b[0m         name\u001b[39m=\u001b[39mmodel_name \u001b[39mif\u001b[39;00m model_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m model_class_name,\n\u001b[1;32m    267\u001b[0m         description\u001b[39m=\u001b[39mmodel_description,\n\u001b[1;32m    268\u001b[0m         progress\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    269\u001b[0m     )\n\u001b[1;32m    270\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_trace.py:750\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mreturn\u001b[39;00m func\n\u001b[1;32m    749\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m--> 750\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    751\u001b[0m         func,\n\u001b[1;32m    752\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mforward\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_inputs},\n\u001b[1;32m    753\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    754\u001b[0m         check_trace,\n\u001b[1;32m    755\u001b[0m         wrap_check_inputs(check_inputs),\n\u001b[1;32m    756\u001b[0m         check_tolerance,\n\u001b[1;32m    757\u001b[0m         strict,\n\u001b[1;32m    758\u001b[0m         _force_outplace,\n\u001b[1;32m    759\u001b[0m         _module_class,\n\u001b[1;32m    760\u001b[0m     )\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    763\u001b[0m     \u001b[39mhasattr\u001b[39m(func, \u001b[39m\"\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    764\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule)\n\u001b[1;32m    765\u001b[0m     \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m ):\n\u001b[1;32m    767\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    768\u001b[0m         func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m,\n\u001b[1;32m    769\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m: example_inputs},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m         _module_class,\n\u001b[1;32m    777\u001b[0m     )\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_trace.py:967\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    963\u001b[0m     argument_names \u001b[39m=\u001b[39m get_callable_argument_names(func)\n\u001b[1;32m    965\u001b[0m example_inputs \u001b[39m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m--> 967\u001b[0m module\u001b[39m.\u001b[39;49m_c\u001b[39m.\u001b[39;49m_create_method_from_trace(\n\u001b[1;32m    968\u001b[0m     method_name,\n\u001b[1;32m    969\u001b[0m     func,\n\u001b[1;32m    970\u001b[0m     example_inputs,\n\u001b[1;32m    971\u001b[0m     var_lookup_fn,\n\u001b[1;32m    972\u001b[0m     strict,\n\u001b[1;32m    973\u001b[0m     _force_outplace,\n\u001b[1;32m    974\u001b[0m     argument_names,\n\u001b[1;32m    975\u001b[0m )\n\u001b[1;32m    976\u001b[0m check_trace_method \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_c\u001b[39m.\u001b[39m_get_method(method_name)\n\u001b[1;32m    978\u001b[0m \u001b[39m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/nn/modules/module.py:1118\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1118\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1119\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Projects/bastionlab/client/src/bastionlab/torch/utils.py:393\u001b[0m, in \u001b[0;36mMultipleOutputWrapper.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 393\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    394\u001b[0m     \u001b[39mreturn\u001b[39;00m output[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput]\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:747\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    745\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 747\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    748\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    749\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    750\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    751\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    752\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    753\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    754\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    755\u001b[0m )\n\u001b[1;32m    756\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    757\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/nn/modules/module.py:1118\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1118\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1119\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:566\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    563\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    565\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[1;32m    568\u001b[0m     x\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m    569\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    574\u001b[0m )\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/nn/modules/module.py:1118\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1118\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1119\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:132\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    129\u001b[0m word_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mword_embeddings(input_ids)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    130\u001b[0m position_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embeddings(position_ids)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m embeddings \u001b[39m=\u001b[39m word_embeddings \u001b[39m+\u001b[39;49m position_embeddings  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m    134\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embeddings)  \u001b[39m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (106) must match the size of tensor b (250) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "from bastionlab.torch.optimizer_config import Adam\n",
    "remote_learner = connection.client.torch.RemoteLearner(\n",
    "    model,\n",
    "    remote_datasets[0],\n",
    "    max_batch_size=2,\n",
    "    loss=\"cross_entropy\",\n",
    "    optimizer=Adam(lr=5e-5),\n",
    "    model_name=\"DistilBERT\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"0\": [1, 2, 3, 4, 5, 6]\n",
    "    }\n",
    ")\n",
    "\n",
    "df.get_column(\"0\").reshape([2,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfb725626286d8c8fc5334ffe77697f720dc23e64d3046271825a5556b528e7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
