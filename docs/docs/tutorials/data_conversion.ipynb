{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bastionlab import Identity\n",
    "\n",
    "# Create `Identity` for Data owner.\n",
    "data_owner = Identity.create(\"data_owner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-14 10:08:03--  https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 60302 (59K) [text/plain]\n",
      "Saving to: ‘titanic.csv’\n",
      "\n",
      "titanic.csv         100%[===================>]  58.89K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2022-12-14 10:08:03 (4.46 MB/s) - ‘titanic.csv’ saved [60302/60302]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_csv(\"titanic.csv\")\n",
    "\n",
    "from bastionlab import Connection\n",
    "\n",
    "connection = Connection(\"localhost\")\n",
    "\n",
    "from bastionlab.polars.policy import Policy, Aggregation, Log\n",
    "\n",
    "policy = Policy(safe_zone=Aggregation(min_agg_size=10), unsafe_handling=Log())\n",
    "rdf = connection.client.polars.send_df(df, policy=policy, sanitized_columns=[\"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rdf = rdf.to_dataset(\n",
    "    [\"Name\", \"Survived\"],\n",
    "    \"Age\",\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"distilbert-base-uncased\",\n",
    ")\n",
    "\n",
    "# _rdf.fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_datasets = connection.client.torch.list_remote_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/kbamponsem/base/lib/python3.8/site-packages (4.19.2)\n",
      "Requirement already satisfied: requests in /home/kbamponsem/base/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/kbamponsem/base/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kbamponsem/base/lib/python3.8/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/kbamponsem/base/lib/python3.8/site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kbamponsem/base/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/kbamponsem/base/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /home/kbamponsem/base/lib/python3.8/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/kbamponsem/base/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/kbamponsem/base/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kbamponsem/base/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/kbamponsem/base/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/kbamponsem/base/lib/python3.8/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kbamponsem/base/lib/python3.8/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kbamponsem/base/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/kbamponsem/base/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from bastionlab.torch.utils import MultipleOutputWrapper\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    torchscript=True,\n",
    ")\n",
    "model = MultipleOutputWrapper(model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have to specify either input_ids or inputs_embeds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotSupportedError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Projects/bastionlab/client/src/bastionlab/torch/learner.py:228\u001b[0m, in \u001b[0;36mRemoteLearner.__init__\u001b[0;34m(self, client, model, remote_dataset, loss, max_batch_size, optimizer, device, max_grad_norm, metric_eps_per_batch, model_name, model_description, expand, progress)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m     model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mscript(model)\n\u001b[1;32m    229\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_script.py:1286\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1285\u001b[0m     obj \u001b[39m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1286\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49mcreate_script_module(\n\u001b[1;32m   1287\u001b[0m         obj, torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49m_recursive\u001b[39m.\u001b[39;49minfer_methods_to_compile\n\u001b[1;32m   1288\u001b[0m     )\n\u001b[1;32m   1290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_recursive.py:458\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    457\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[39m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 458\u001b[0m \u001b[39mreturn\u001b[39;00m create_script_module_impl(nn_module, concrete_type, stubs_fn)\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_recursive.py:470\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    469\u001b[0m cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_create_module_with_type(concrete_type\u001b[39m.\u001b[39mjit_type)\n\u001b[0;32m--> 470\u001b[0m method_stubs \u001b[39m=\u001b[39m stubs_fn(nn_module)\n\u001b[1;32m    471\u001b[0m property_stubs \u001b[39m=\u001b[39m get_property_stubs(nn_module)\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_recursive.py:739\u001b[0m, in \u001b[0;36minfer_methods_to_compile\u001b[0;34m(nn_module)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[39mfor\u001b[39;00m method \u001b[39min\u001b[39;00m uniqued_methods:\n\u001b[0;32m--> 739\u001b[0m     stubs\u001b[39m.\u001b[39mappend(make_stub_from_method(nn_module, method))\n\u001b[1;32m    740\u001b[0m \u001b[39mreturn\u001b[39;00m overload_stubs \u001b[39m+\u001b[39m stubs\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_recursive.py:66\u001b[0m, in \u001b[0;36mmake_stub_from_method\u001b[0;34m(nn_module, method_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m# Make sure the name present in the resulting AST will match the name\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# requested here. The only time they don't match is if you do something\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m# like:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m# In this case, the actual function object will have the name `_forward`,\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m# even though we requested a stub for `forward`.\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[39mreturn\u001b[39;00m make_stub(func, method_name)\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_recursive.py:51\u001b[0m, in \u001b[0;36mmake_stub\u001b[0;34m(func, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m rcb \u001b[39m=\u001b[39m _jit_internal\u001b[39m.\u001b[39mcreateResolutionCallbackFromClosure(func)\n\u001b[0;32m---> 51\u001b[0m ast \u001b[39m=\u001b[39m get_jit_def(func, name, self_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mRecursiveScriptModule\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     52\u001b[0m \u001b[39mreturn\u001b[39;00m ScriptMethodStub(rcb, ast, func)\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/frontend.py:265\u001b[0m, in \u001b[0;36mget_jit_def\u001b[0;34m(fn, def_name, self_name, is_classmethod)\u001b[0m\n\u001b[1;32m    263\u001b[0m     pdt_arg_types \u001b[39m=\u001b[39m type_trace_db\u001b[39m.\u001b[39mget_args_types(qualname)\n\u001b[0;32m--> 265\u001b[0m \u001b[39mreturn\u001b[39;00m build_def(parsed_def\u001b[39m.\u001b[39;49mctx, fn_def, type_line, def_name, self_name\u001b[39m=\u001b[39;49mself_name, pdt_arg_types\u001b[39m=\u001b[39;49mpdt_arg_types)\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/frontend.py:303\u001b[0m, in \u001b[0;36mbuild_def\u001b[0;34m(ctx, py_def, type_line, def_name, self_name, pdt_arg_types)\u001b[0m\n\u001b[1;32m    299\u001b[0m r \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mmake_range(py_def\u001b[39m.\u001b[39mlineno \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(py_def\u001b[39m.\u001b[39mdecorator_list),\n\u001b[1;32m    300\u001b[0m                    py_def\u001b[39m.\u001b[39mcol_offset,\n\u001b[1;32m    301\u001b[0m                    py_def\u001b[39m.\u001b[39mcol_offset \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdef\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 303\u001b[0m param_list \u001b[39m=\u001b[39m build_param_list(ctx, py_def\u001b[39m.\u001b[39;49margs, self_name, pdt_arg_types)\n\u001b[1;32m    304\u001b[0m return_type \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/frontend.py:327\u001b[0m, in \u001b[0;36mbuild_param_list\u001b[0;34m(ctx, py_args, self_name, pdt_arg_types)\u001b[0m\n\u001b[1;32m    326\u001b[0m     ctx_range \u001b[39m=\u001b[39m ctx\u001b[39m.\u001b[39mmake_range(expr\u001b[39m.\u001b[39mlineno, expr\u001b[39m.\u001b[39mcol_offset \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, expr\u001b[39m.\u001b[39mcol_offset \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(expr\u001b[39m.\u001b[39marg))\n\u001b[0;32m--> 327\u001b[0m     \u001b[39mraise\u001b[39;00m NotSupportedError(ctx_range, _vararg_kwarg_err)\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m py_args\u001b[39m.\u001b[39mvararg \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotSupportedError\u001b[0m: Compiled functions can't take variable number of arguments or use keyword-only arguments with defaults:\n  File \"/home/kbamponsem/Projects/bastionlab/client/src/bastionlab/torch/utils.py\", line 392\n    def forward(self, *args, **kwargs) -> Tensor:\n                              ~~~~~~~ <--- HERE\n        output = self.inner.forward(*args, **kwargs)\n        return output[self.output]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbastionlab\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer_config\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m client \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mtorch\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m remote_learner \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mRemoteLearner(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     remote_datasets[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     max_batch_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     loss\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcross_entropy\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49mAdam(lr\u001b[39m=\u001b[39;49m\u001b[39m5e-5\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDistilBERT\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/bastionlab/client/src/bastionlab/torch/client.py:227\u001b[0m, in \u001b[0;36mBastionLabTorch.RemoteLearner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39m\"\"\"Returns a RemoteLearner object encapsulating a model and hyperparameters for\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39mtraining and testing on the remote server and that uses this client to communicate with the server.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m    **kwargs: all keyword arguments are forwarded to the `RemoteDataLoader` constructor.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlearner\u001b[39;00m \u001b[39mimport\u001b[39;00m RemoteLearner\n\u001b[0;32m--> 227\u001b[0m \u001b[39mreturn\u001b[39;00m RemoteLearner(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/bastionlab/client/src/bastionlab/torch/learner.py:230\u001b[0m, in \u001b[0;36mRemoteLearner.__init__\u001b[0;34m(self, client, model, remote_dataset, loss, max_batch_size, optimizer, device, max_grad_norm, metric_eps_per_batch, model_name, model_description, expand, progress)\u001b[0m\n\u001b[1;32m    228\u001b[0m         model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mscript(model)\n\u001b[1;32m    229\u001b[0m     \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> 230\u001b[0m         model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mtrace(  \u001b[39m# Compile the model with the tracing strategy\u001b[39;49;00m\n\u001b[1;32m    231\u001b[0m             \u001b[39m# Wrapp the model to use the first output only (and drop the others)\u001b[39;49;00m\n\u001b[1;32m    232\u001b[0m             model,\n\u001b[1;32m    233\u001b[0m             [x\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m remote_dataset\u001b[39m.\u001b[39;49mtrace_input],\n\u001b[1;32m    234\u001b[0m         )\n\u001b[1;32m    235\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_ref \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39msend_model(\n\u001b[1;32m    236\u001b[0m         model,\n\u001b[1;32m    237\u001b[0m         name\u001b[39m=\u001b[39mmodel_name \u001b[39mif\u001b[39;00m model_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m model_class_name,\n\u001b[1;32m    238\u001b[0m         description\u001b[39m=\u001b[39mmodel_description,\n\u001b[1;32m    239\u001b[0m         progress\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    240\u001b[0m     )\n\u001b[1;32m    241\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_trace.py:750\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mreturn\u001b[39;00m func\n\u001b[1;32m    749\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m--> 750\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    751\u001b[0m         func,\n\u001b[1;32m    752\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mforward\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_inputs},\n\u001b[1;32m    753\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    754\u001b[0m         check_trace,\n\u001b[1;32m    755\u001b[0m         wrap_check_inputs(check_inputs),\n\u001b[1;32m    756\u001b[0m         check_tolerance,\n\u001b[1;32m    757\u001b[0m         strict,\n\u001b[1;32m    758\u001b[0m         _force_outplace,\n\u001b[1;32m    759\u001b[0m         _module_class,\n\u001b[1;32m    760\u001b[0m     )\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    763\u001b[0m     \u001b[39mhasattr\u001b[39m(func, \u001b[39m\"\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    764\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule)\n\u001b[1;32m    765\u001b[0m     \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m ):\n\u001b[1;32m    767\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    768\u001b[0m         func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m,\n\u001b[1;32m    769\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m: example_inputs},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m         _module_class,\n\u001b[1;32m    777\u001b[0m     )\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/jit/_trace.py:967\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    963\u001b[0m     argument_names \u001b[39m=\u001b[39m get_callable_argument_names(func)\n\u001b[1;32m    965\u001b[0m example_inputs \u001b[39m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m--> 967\u001b[0m module\u001b[39m.\u001b[39;49m_c\u001b[39m.\u001b[39;49m_create_method_from_trace(\n\u001b[1;32m    968\u001b[0m     method_name,\n\u001b[1;32m    969\u001b[0m     func,\n\u001b[1;32m    970\u001b[0m     example_inputs,\n\u001b[1;32m    971\u001b[0m     var_lookup_fn,\n\u001b[1;32m    972\u001b[0m     strict,\n\u001b[1;32m    973\u001b[0m     _force_outplace,\n\u001b[1;32m    974\u001b[0m     argument_names,\n\u001b[1;32m    975\u001b[0m )\n\u001b[1;32m    976\u001b[0m check_trace_method \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_c\u001b[39m.\u001b[39m_get_method(method_name)\n\u001b[1;32m    978\u001b[0m \u001b[39m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/nn/modules/module.py:1118\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1118\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1119\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/Projects/bastionlab/client/src/bastionlab/torch/utils.py:393\u001b[0m, in \u001b[0;36mMultipleOutputWrapper.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 393\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    394\u001b[0m     \u001b[39mreturn\u001b[39;00m output[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput]\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:747\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    745\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 747\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistilbert(\n\u001b[1;32m    748\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    749\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    750\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    751\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    752\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    753\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    754\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    755\u001b[0m )\n\u001b[1;32m    756\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    757\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/torch/nn/modules/module.py:1118\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         recording_scopes \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1118\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1119\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m     \u001b[39mif\u001b[39;00m recording_scopes:\n",
      "File \u001b[0;32m~/base/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:555\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    553\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    557\u001b[0m device \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mdevice \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m inputs_embeds\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify either input_ids or inputs_embeds"
     ]
    }
   ],
   "source": [
    "from bastionlab.torch.optimizer_config import Adam\n",
    "\n",
    "client = connection.client.torch\n",
    "remote_learner = client.RemoteLearner(\n",
    "    model,\n",
    "    remote_datasets[0],\n",
    "    max_batch_size=2,\n",
    "    loss=\"cross_entropy\",\n",
    "    optimizer=Adam(lr=5e-5),\n",
    "    model_name=\"DistilBERT\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfb725626286d8c8fc5334ffe77697f720dc23e64d3046271825a5556b528e7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
