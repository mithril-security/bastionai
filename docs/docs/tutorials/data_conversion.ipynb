{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bastionlab import Identity\n",
    "\n",
    "# Create `Identity` for Data owner.\n",
    "data_owner = Identity.create(\"data_owner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python datagen.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "!unzip smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==0.13.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbamponsem/base/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from bastionlab.polars.policy import Policy, Aggregation, Log\n",
    "from bastionlab.polars import train_test_split\n",
    "import polars as pl\n",
    "from bastionlab import Connection\n",
    "\n",
    "file_path = \"./SMSSpamCollection\"\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "with open(file_path) as f:\n",
    "    for line in f.readlines():\n",
    "        split = line.split(\"\\t\")\n",
    "        labels.append(1 if split[0] == \"spam\" else 0)\n",
    "        texts.append(split[1])\n",
    "df = pl.DataFrame({\"label\": labels, \"text\": texts})\n",
    "\n",
    "connection = Connection(\"localhost\")\n",
    "\n",
    "\n",
    "rdf = connection.client.polars.send_df(df.limit(30))\n",
    "\n",
    "ratio = 0.25\n",
    "train_rdf, test_rdf = train_test_split(rdf, test_size=ratio, shuffle=True)\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer.enable_truncation(max_length=32)\n",
    "tokenizer.enable_padding(length=32)\n",
    "\n",
    "train_rds = (\n",
    "    train_rdf.convert([\"text\"], tokenizer.to_str())\n",
    "    .collect()\n",
    "    .to_dataset([\"text_ids\", \"text_mask\"], \"label\")\n",
    ")\n",
    "test_rds = (\n",
    "    test_rdf.convert([\"text\"], tokenizer.to_str())\n",
    "    .collect()\n",
    "    .to_dataset([\"text_ids\", \"text_mask\"], \"label\")\n",
    ")\n",
    "\n",
    "\n",
    "train_rds._set_test_dataset(test_rds.train_dataset_ref)\n",
    "test_rds.set_train_dataset(train_dataset=train_rds.train_dataset_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from bastionlab.torch.utils import MultipleOutputWrapper\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    torchscript=True,\n",
    ")\n",
    "model = MultipleOutputWrapper(model, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BastionLabTorch' object has no attribute 'client'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m remote_datasets \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlist_remote_datasets()\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(remote_datasets)\n\u001b[1;32m      4\u001b[0m remote_datasets[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtrace_input\n",
      "File \u001b[0;32m~/Projects/bastionlab/client/src/bastionlab/torch/client.py:242\u001b[0m, in \u001b[0;36mBastionLabTorch.list_remote_datasets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlist_remote_datasets\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39m\"\u001b[39m\u001b[39mRemoteDataset\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    240\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlearner\u001b[39;00m \u001b[39mimport\u001b[39;00m RemoteDataset\n\u001b[0;32m--> 242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39mrefresh_session_if_needed()\n\u001b[1;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m RemoteDataset\u001b[39m.\u001b[39mlist_available(\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BastionLabTorch' object has no attribute 'client'"
     ]
    }
   ],
   "source": [
    "remote_datasets = connection.client.torch.list_remote_datasets()\n",
    "print(remote_datasets)\n",
    "\n",
    "remote_datasets[0].trace_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending DistilBERT: 100%|████████████████████| 268M/268M [00:06<00:00, 39.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "from bastionlab.torch.optimizer_config import Adam\n",
    "\n",
    "remote_learner = connection.client.torch.RemoteLearner(\n",
    "    model,\n",
    "    remote_datasets[0],\n",
    "    max_batch_size=2,\n",
    "    loss=\"cross_entropy\",\n",
    "    optimizer=Adam(lr=5e-5),\n",
    "    model_name=\"DistilBERT\",\n",
    "    expand=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - train:   1%|                    | 12/2090 [00:23<39:52,  1.15s/batch, cross_entropy=0.0000 (+/- 1553.7606)]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/kbamponsem/Projects/bastionlab/docs/docs/tutorials/data_conversion.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m remote_learner\u001b[39m.\u001b[39;49mfit(nb_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, eps\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Projects/bastionlab/client/src/bastionlab/torch/learner.py:467\u001b[0m, in \u001b[0;36mRemoteLearner.fit\u001b[0;34m(self, nb_epochs, eps, batch_size, max_grad_norm, lr, metric_eps, timeout, poll_delay, per_n_epochs_checkpoint, per_n_steps_checkpoint, resume)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39m\"\"\"Fits the uploaded model to the training dataset with given hyperparameters.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \n\u001b[1;32m    443\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39m    poll_delay: Delay in seconds between two polling requests for the loss.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m run \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mtrain(\n\u001b[1;32m    455\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_config(\n\u001b[1;32m    456\u001b[0m         nb_epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    465\u001b[0m     )\n\u001b[1;32m    466\u001b[0m )\n\u001b[0;32m--> 467\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll_metric(\n\u001b[1;32m    468\u001b[0m     run,\n\u001b[1;32m    469\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss,\n\u001b[1;32m    470\u001b[0m     train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    471\u001b[0m     timeout\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(timeout \u001b[39m/\u001b[39;49m poll_delay),\n\u001b[1;32m    472\u001b[0m     poll_delay\u001b[39m=\u001b[39;49mpoll_delay,\n\u001b[1;32m    473\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/bastionlab/client/src/bastionlab/torch/learner.py:389\u001b[0m, in \u001b[0;36mRemoteLearner._poll_metric\u001b[0;34m(self, run, name, train, timeout, poll_delay)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mappend(metric)\n\u001b[1;32m    388\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m     sleep(poll_delay)\n\u001b[1;32m    390\u001b[0m     prev_batch \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39mbatch\n\u001b[1;32m    391\u001b[0m     prev_epoch \u001b[39m=\u001b[39m metric\u001b[39m.\u001b[39mepoch\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "remote_learner.fit(nb_epochs=2, eps=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfb725626286d8c8fc5334ffe77697f720dc23e64d3046271825a5556b528e7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
