{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune DistilBERT for binary classification on the SMS Spam Collection\n",
    "__________________________\n",
    "\n",
    "This can be useful, for instance, when one wants to leverage large pre-trained models on a smaller private dataset, for instance, medical or financial records, and ensure data privacy regarding users' data.\n",
    "\n",
    "BastionLabTorch is intended for scenarios where we have a data owner, for instance, a hospital, wanting to have third parties train models on their data, e.g. a startup, potentially on untrusted infrastructures, such as in the Cloud.\n",
    "\n",
    "The strength of BastionLabTorch is that the data owner can have a high level of protection on data shared to a remote enclave hosted in the Cloud, and operated by the startup, thanks to memory isolation and encryption, and remote attestation from the use of secure enclaves.\n",
    "\n",
    "In this notebook, we will illustrate how BastionLab works. We will use the publicly available dataset [SMS Spam Collection](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset) to finetune a DistilBERT model on a classification task, to predict whether an email is spam or not.\n",
    "\n",
    "In this guide, we will cover two phases:\n",
    "- The offline phase, in which the data owner prepares the dataset and the data scientist prepares the model.\n",
    "- The online phase, in which dataset and model are uploaded to the secure enclave. In the enclave, the uploaded model will be trained on the dataset. The data scientist can pull the weights once the training is over.\n",
    "\n",
    "We largely followed [this tutorial](https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894) to prepare the data and pre-train the model we'll use in this example.\n",
    "\n",
    "## Pre-requisites\n",
    "__________________________\n",
    "\n",
    "We need to have installed: \n",
    "- [BastionLab](https://bastionlab.readthedocs.io/en/latest/docs/getting-started/installation/)\n",
    "- Hugging Face's [Transformers library](https://huggingface.co/docs/transformers/installation)\n",
    "- [Polars](https://www.pola.rs/)\n",
    "- [IPython kernel](https://ipython.readthedocs.io/en/stable/install/kernel_install.html) for Jupyter\n",
    "- [Jupyter Widgets](https://ipywidgets.readthedocs.io/en/7.x/user_install.html) to enable notebooks extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bastionlab\n",
    "!pip install transformers polars ipykernel ipywidgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline phase - Model and dataset preparation\n",
    "__________________________________\n",
    "\n",
    "In this section, data owner and data scientist will prepare their data and model so that they are ready-to-use for the training in BastionLab.\n",
    "\n",
    "### Data owner's side: preparing the dataset\n",
    "\n",
    "In this example, our data owner wants a third party data scientist to train an AI model to detect spam from emails.\n",
    "\n",
    "Of course, in a real-world scenario, the data owner already posesses the data, but here, we will need to download one! We'll get the SPAM collection dataset and unzip it by running the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https: // archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "!unzip smsspamcollection.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload the dataset to the BastionLab server, the data owner will need to prepare their dataset and make it available in a PyTorch `DataSet` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe td {\n",
       "        white-space: pre;\n",
       "    }\n",
       "\n",
       "    .dataframe td {\n",
       "        padding-top: 0;\n",
       "    }\n",
       "\n",
       "    .dataframe td {\n",
       "        padding-bottom: 0;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\" >\n",
       "<small>shape: (5, 2)</small>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>\n",
       "label\n",
       "</th>\n",
       "<th>\n",
       "text\n",
       "</th>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "i64\n",
       "</td>\n",
       "<td>\n",
       "str\n",
       "</td>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "<td>\n",
       "&quot;Go until juron...\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "<td>\n",
       "&quot;Ok lar... Joki...\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "1\n",
       "</td>\n",
       "<td>\n",
       "&quot;Free entry in ...\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "<td>\n",
       "&quot;U dun say so e...\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "<td>\n",
       "&quot;Nah I don&#x27;t th...\n",
       "</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌───────┬─────────────────────────────────────┐\n",
       "│ label ┆ text                                │\n",
       "│ ---   ┆ ---                                 │\n",
       "│ i64   ┆ str                                 │\n",
       "╞═══════╪═════════════════════════════════════╡\n",
       "│ 0     ┆ Go until jurong point, crazy.. A... │\n",
       "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
       "│ 0     ┆ Ok lar... Joking wif u oni...       │\n",
       "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
       "│ 1     ┆ Free entry in 2 a wkly comp to w... │\n",
       "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
       "│ 0     ┆ U dun say so early hor... U c al... │\n",
       "├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
       "│ 0     ┆ Nah I don't think he goes to usf... │\n",
       "└───────┴─────────────────────────────────────┘"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "file_path = \"./SMSSpamCollection\"\n",
    "\n",
    "# Read CSV file using Polars and rename columns with `text`, `label`\n",
    "df = pl.read_csv(file_path, has_header=False, sep=\"\\t\", new_columns=[\"label\", \"text\"])\n",
    "\n",
    "# Transform `spam` labels to `1` and `0` for any other column label\n",
    "df = df.with_column(\n",
    "    pl.when(pl.col(\"label\") == \"spam\").then(1).otherwise(0).alias(\"label\")\n",
    ")\n",
    "\n",
    "# View the first few elements of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data owner also needs to preprocess the data. We'll use a `DistilBertTokenizer` to obtain tensors ready to be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "# The Distilbert Tokenizer is loaded from HuggingFace's repository.\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# These two variables store the input_ids and attention_mask per each sentence.\n",
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "# The DataFrame is converted to a dictionary with keys (`text` and `label`).\n",
    "df_dict = df.to_dict(as_series=False)\n",
    "samples = df_dict[\"text\"]\n",
    "labels = df_dict[\"label\"]\n",
    "\n",
    "# Each row (in other words, sentence) in the DataFrame is passed to the tokenizer\n",
    "# and we expect two (2) items for each row {input_ids: [], attention_mask: []}.\n",
    "# Each is appended the to token_id and attention_mask respectively.\n",
    "\n",
    "for sample in samples:\n",
    "    encoding_dict = tokenizer.encode_plus(\n",
    "        sample[0],\n",
    "        add_special_tokens=True,\n",
    "        max_length=32,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    token_id.append(encoding_dict[\"input_ids\"])\n",
    "    attention_masks.append(encoding_dict[\"attention_mask\"])\n",
    "\n",
    "# We create a single tensor from the List[Tensor].\n",
    "token_id = torch.cat(token_id, dim=0).to(dtype=torch.int64)\n",
    "\n",
    "# We create a single tensor from the List[Tensor].\n",
    "attention_masks = torch.cat(attention_masks, dim=0).to(dtype=torch.int64)\n",
    "\n",
    "# Here, we convert List[int] into a Tensor.\n",
    "labels = torch.tensor(labels, dtype=torch.int64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the training process faster in this demonstration, we'll only take a subset of the dataset, but you can choose to take the whole dataset if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ratio of the validation data set for the trainer.\n",
    "test_ratio = 0.2\n",
    "\n",
    "limit = 64\n",
    "\n",
    "nb_samples = len(token_id)\n",
    "\n",
    "# Generates an ndarray of indexes in the range [0, `nb_samples`] with `0`\n",
    "# and `nb_samples` included\n",
    "idx = np.arange(nb_samples)\n",
    "\n",
    "# Shuffle the generated indexes\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "# Extract `nb_samples` starting from `test_ratio * nb_samples` for the train_idx\n",
    "train_idx = idx[int(test_ratio * nb_samples) :][:limit]\n",
    "\n",
    "# Extract `nb_samples` starting from where `test_ratio * nb_samples` ends\n",
    "# for the test_idx\n",
    "test_idx = idx[: int(test_ratio * nb_samples)][:limit]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our training and validation `TensorDataset` objects. We'll use them to wrap our `Tensor` objects into a PyTorch `DataSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bastionlab.torch.utils import TensorDataset\n",
    "\n",
    "# The training tensor is converted to a TensorDataset because the\n",
    "# BastionLab Torch API only accepts datasets.\n",
    "train_set = TensorDataset(\n",
    "    [token_id[train_idx], attention_masks[train_idx]], labels[train_idx]\n",
    ")\n",
    "\n",
    "# The validation tensor is also converted to a TensorDataset because\n",
    "# the BastionLab Torch API only accepts datasets.\n",
    "validation_set = TensorDataset(\n",
    "    [token_id[test_idx], attention_masks[test_idx]], labels[test_idx]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scientist's side: preparing the model\n",
    "\n",
    "On his side, the data scientist must prepare the DistilBERT language model. \n",
    "\n",
    "One important thing to know about BastionLab is that it supports models with an arbitrary number of inputs, but it only supports models with a single output. This is the first step we need to address as Hugging Face's models typically have several outputs (*logits, loss, etc*).\n",
    "\n",
    "We'll use BastionLab's utility wrapper to select only one output of the model. In our case: the one that corresponds with the logits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# The class MultipleOutputWrapper is used to select one (1) of\n",
    "# the model outputs.\n",
    "# In this example, we select the `loss` output. It wraps around\n",
    "# a torch Module and redirects calls to the forward method to the\n",
    "# inner forward method.\n",
    "class MultipleOutputWrapper(nn.Module):\n",
    "    \"\"\"Utility wrapper to select one output of a model with multiple outputs.\n",
    "\n",
    "    Args:\n",
    "        module: A model with more than one output.\n",
    "        output: Index of the output to retain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, module: nn.Module, output: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        self.inner = module\n",
    "        self.output = output\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> torch.Tensor:\n",
    "        output = self.inner.forward(*args, **kwargs)\n",
    "        return output[self.output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# We load the Distilbert Classifier from HuggingFace's repository\n",
    "# and enable torchscript support.This makes it possible to trace\n",
    "# the model and send it to the BastionLab Torch service since it\n",
    "# only accepts TorchScript models.\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    torchscript=True,\n",
    ")\n",
    "model = MultipleOutputWrapper(\n",
    "    model, 0\n",
    ")  # MultipleOutputWrapper() can be loaded from bastionlab.torch.utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online phase - dataset and model upload and training\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that that both dataset and model are prepared, we can upload them securely to the secure enclave for the training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data owner's side: uploading the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will connect to the BastionLab Torch instance using our `Connection` library. \n",
    "\n",
    "Once connected, we'll use the `RemoteDataset()` method to upload the datasets to the BastionLab Torch service. The method needs us to provide a name, and set a Differential Privacy budget. Here we put `1 000 000` an arbitrary number, but as a rule of thumb, it should be much lower, such as 4 or 8.\n",
    "\n",
    "> *To learn more about Differential Privacy and why it's important you use it, you [can read this article](https://en.wikipedia.org/wiki/Differential_privacy#:~:text=Differential%20privacy%20(DP)%20is%20a,about%20individuals%20in%20the%20dataset.).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending SMSSpamCollection: 100%|████████████████████| 35.7k/35.7k [00:00<00:00, 38.2MB/s]\n",
      "Sending SMSSpamCollection (test): 100%|████████████████████| 35.7k/35.7k [00:00<00:00, 34.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "from bastionlab import Connection\n",
    "\n",
    "# The Data owner privately uploads their model online\n",
    "client = Connection(\"localhost\").client.torch\n",
    "\n",
    "remote_dataset = client.RemoteDataset(\n",
    "    train_set, validation_set, name=\"SMSSpamCollection\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data scientist's side: uploading the model and trigger training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's finally time to train the model! \n",
    "\n",
    "The data scientist will use the `list_remote_datasets` endpoint to get a list of the available datasets on the server that they'll be able to use for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SMSSpamCollection (e4377764d92780aca061fb21f3afcb8b3b0d44cc30a27d9e3123d92eb63259e1): size=64, desc=N/A']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Connection(\"localhost\").client.torch\n",
    "\n",
    "# Fetches the list of all `RemoteDataset` on the BastionLab Torch service\n",
    "remote_datasets = client.list_remote_datasets()\n",
    "\n",
    "# Here, we print the list of the available RemoteDatasets on the BastionLab Torch service\n",
    "# It will display in this form `[\"(Name): nb_samples=int, dtype=str\"]`\n",
    "[str(ds) for ds in remote_datasets]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset uploaded previously is available as a `RemoteDataset ` object. It is a pointer to the remote dataset uploaded previously, **that contains only metadata and nothing else**. This allows the data scientist to play with remote datasets without users' data being exposed in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bastionlab.torch.learner.RemoteDataset at 0x7ff880525420>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, only the first element of the dataset is printed\n",
    "remote_datasets[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To send the model and necessary training parameters to the server, we'll use the `RemoteLearner()` method. \n",
    "\n",
    "To start training, we'll call the `fit` method on the `RemoteLearner` object with an appropriate number of epochs and Differential Privacy budget.\n",
    "\n",
    ">  An epoch is when all the training data is used at once and is defined as the total number of iterations of all the training data in one cycle for training the machine learning model\n",
    "\n",
    "Then we'll test the model directly on the server with the `test()` method.\n",
    "\n",
    "\n",
    "> Note that behind the scenes, a DP-SGD training loop will be used. To learn more about DP-SGD, [click here](https://arxiv.org/pdf/1607.00133.pdf)\n",
    "\n",
    "Finally, and it's the last step of this tutorial, we'll retrieve a local copy of the trained model once the training is complete. To do so, we'll use the `get_model()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending DistilBERT: 100%|████████████████████| 268M/268M [00:03<00:00, 71.2MB/s] \n",
      "Epoch 1/2 - train: 100%|████████████████████| 32/32 [00:13<00:00,  2.46batch/s, cross_entropy=0.5558 (+/- 0.0000)] \n",
      "Epoch 2/2 - train: 100%|████████████████████| 32/32 [00:11<00:00,  2.70batch/s, cross_entropy=0.5000 (+/- 0.0000)]\n",
      "Epoch 1/1 - test: 100%|████████████████████| 32/32 [00:01<00:00, 26.33batch/s, accuracy=0.8874 (+/- 0.0000)] \n"
     ]
    }
   ],
   "source": [
    "from bastionlab.torch.optimizer_config import Adam\n",
    "\n",
    "# Torch is selected from the created client connection\n",
    "# BastionLab has multiple services (Polars, Torch, etc)\n",
    "client = Connection(\"localhost\").client.torch\n",
    "\n",
    "# A remote learner is created with the code below. It contains\n",
    "# the DistilBERT model, loss type, the optimizer to use, as well as\n",
    "# the training dataset to use.\n",
    "remote_learner = client.RemoteLearner(\n",
    "    model,\n",
    "    remote_datasets[0],\n",
    "    max_batch_size=2,\n",
    "    loss=\"cross_entropy\",\n",
    "    optimizer=Adam(lr=5e-5),\n",
    "    model_name=\"DistilBERT\",\n",
    ")\n",
    "\n",
    "# fit() is called to remotely trigger training\n",
    "remote_learner.fit(nb_epochs=2)\n",
    "\n",
    "# The trained model is tested with the `accuracy` metric.\n",
    "remote_learner.test(metric=\"accuracy\")\n",
    "\n",
    "# The trained model is fetched using the get_model() method\n",
    "trained_model = remote_learner.get_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b62e9ff4e56dfa5f055fe055066e5a02d5f9234d63201164ad52aa7acce06a4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
