{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning DistilBERT for binary classification on the SMS Spam Collection\n",
    "----------------------------------------------------------------------------\n",
    "We will see in this notebook how BastionLabTorch can be used to fine-tune a pre-trained DistilBERT model on a small dataset.\n",
    "This can be useful, for instance, when one wants to leverage large pre-trained models on a smaller private dataset, for instance, medical or financial records, and ensure data privacy regarding users' data.\n",
    "\n",
    "BastionLabTorch is intended for scenarios where we have a data owner, for instance, a hospital, wanting to have third parties train models on their data, e.g. a startup, potentially on untrusted infrastructures, such as in the Cloud.\n",
    "\n",
    "The strength of BastionLabTorch is that the data owner can have a high level of protection on data shared to a remote enclave hosted in the Cloud, and operated by the startup, thanks to memory isolation and encryption, and remote attestation from the use of secure enclaves. \n",
    "\n",
    "In this notebook, we will illustrate how BlindAI works. We will use the publicly available dataset [SMS Spam Collection](https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset) to finetune a DistilBERT model on a classification task, to predict whether an email is spam or not.\n",
    "\n",
    "![](https://github.com/mithril-security/bastionlab/raw/master/docs/assets/bastionlab_torch.png)\n",
    "\n",
    "The workflow is simple:\n",
    "- First, initialize the secure enclave used for training by launching our BastionLabTorch server Docker image. \n",
    "- In an offline phase, the data owner prepares the dataset and the data scientist prepares the model.\n",
    "- In an online phase, the dataset is uploaded to the secure enclave, the data scientist uploads his model inside the enclave, and have it be trained on the previously uploaded dataset, before pulling the weights once training is over.\n",
    "\n",
    "Data preparation and training are largely based on https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894.\n",
    "\n",
    "\n",
    "## Installing dependencies\n",
    "--------------------------\n",
    "\n",
    "### BastionLab\n",
    "\n",
    "You can install BastionLabTorch using our PyPI package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bastionlab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing other packages\n",
    "\n",
    "Let's import all the necessary packages for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers pandas ipykernel ipywidgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing BastionLab Server\n",
    "\n",
    "For testing purposes, BastionLab server has been packaged as a pip wheel. In this tutorial, we will use this package to quickly set up a test server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now import necessary packages and objects:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Offline phase - Model and dataset preparation\n",
    "\n",
    "### Data Owner POV: Preparing the Dataset\n",
    "\n",
    "Here we will suppose we have a data owner who wants a third party data scientist to train an AI model to detect spam from emails.\n",
    "\n",
    "The data owner will have to prepare the dataset first, and have it available in a PyTorch `DataSet` object before uploading it to the BastionLab server.\n",
    "\n",
    "The dataset can be found at https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip. We will download and unzip it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-12 17:02:54--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 203415 (199K) [application/x-httpd-php]\n",
      "Saving to: ‘smsspamcollection.zip.2’\n",
      "\n",
      "smsspamcollection.z 100%[===================>] 198.65K   251KB/s    in 0.8s    \n",
      "\n",
      "2022-12-12 17:02:56 (251 KB/s) - ‘smsspamcollection.zip.2’ saved [203415/203415]\n",
      "\n",
      "Archive:  smsspamcollection.zip\n",
      "replace SMSSpamCollection? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
    "!unzip smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represent a sample, the label comes first followed by a tab and the raw text:\n",
    "```\n",
    "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
    "ham\tOk lar... Joking wif u oni...\n",
    "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
    "```\n",
    "\n",
    "We first load the data from the file into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                    Ok lar... Joking wif u oni...\\n\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"./SMSSpamCollection\"\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "with open(file_path) as f:\n",
    "    for line in f.readlines():\n",
    "        split = line.split(\"\\t\")\n",
    "        labels.append(1 if split[0] == \"spam\" else 0)\n",
    "        texts.append(split[1])\n",
    "df = pd.DataFrame({\"label\": labels, \"text\": texts})\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then preprocess the data using a `DistilBertTokenizer` and we obtain tensors ready to be fed to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "token_id = []\n",
    "attention_masks = []\n",
    "for sample in df.text.values:\n",
    "    encoding_dict = tokenizer.encode_plus(\n",
    "        sample,\n",
    "        add_special_tokens=True,\n",
    "        max_length=32,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    token_id.append(encoding_dict[\"input_ids\"])\n",
    "    attention_masks.append(encoding_dict[\"attention_mask\"])\n",
    "\n",
    "token_id = torch.cat(token_id, dim=0).to(dtype=torch.int64)\n",
    "attention_masks = torch.cat(attention_masks, dim=0).to(dtype=torch.int64)\n",
    "labels = torch.tensor(df.label.values, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will take only a subset of the dataset to make the training process faster. You can choose to take the whole dataset if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_ratio = 0.2\n",
    "limit = 64\n",
    "nb_samples = len(token_id)\n",
    "\n",
    "idx = np.arange(nb_samples)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "train_idx = idx[int(test_ratio * nb_samples) :][:limit]\n",
    "test_idx = idx[: int(test_ratio * nb_samples)][:limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create two `TensorDataset` objects, that will be used to wrap our `Tensor` objects into a PyTorch `DataSet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bastionlab.torch.utils import TensorDataset\n",
    "\n",
    "train_set = TensorDataset(\n",
    "    [token_id[train_idx], attention_masks[train_idx]], labels[train_idx]\n",
    ")\n",
    "\n",
    "test_set = TensorDataset(\n",
    "    [token_id[test_idx], attention_masks[test_idx]], labels[test_idx]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scientist POV: Preparing the model\n",
    "\n",
    "We now turn to preparing the DistilBERT language model. As Hugging Face's models typically have several outputs (logits, loss, etc.), we use Bastion AI's utility wrapper for models with multiple outputs to select only the output that corresponds with the logits. In fact, Bastion AI's server supports models with an arbitrtary number of inputs but only supports models with a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultipleOutputWrapper(nn.Module):\n",
    "    \"\"\"Utility wrapper to select one output of a model with multiple outputs.\n",
    "\n",
    "    Args:\n",
    "        module: A model with more than one output.\n",
    "        output: Index of the output to retain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, module: nn.Module, output: int = 0) -> None:\n",
    "        super().__init__()\n",
    "        self.inner = module\n",
    "        self.output = output\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> torch.Tensor:\n",
    "        output = self.inner.forward(*args, **kwargs)\n",
    "        return output[self.output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    torchscript=True,\n",
    ")\n",
    "model = MultipleOutputWrapper(\n",
    "    model, 0\n",
    ")  # This can be loaded from bastionlab.torch.utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online phase - dataset and model upload and training\n",
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset and model are prepared, we can go to the online phase where the dataset is uploaded securely in the enclave, and the model is sent there for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Owner POV: Uploading the Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will connect to the BastionLab Torch instance using our library. Once connected, we use the `RemoteDataset` method to upload the datasets inside, provide a name, and set a Differential Privacy budget. Here we put an arbitrary number, but as a rule of thumb, the DP budget should be much lower, such as 4 or 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending SMSSpamCollection: 100%|████████████████████| 35.6k/35.6k [00:00<00:00, 8.86MB/s]\n",
      "Sending SMSSpamCollection (test): 100%|████████████████████| 35.6k/35.6k [00:00<00:00, 20.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "from bastionlab import Connection\n",
    "\n",
    "# The Data Owner privately uploads their model online\n",
    "client = Connection(\"localhost\").client.torch\n",
    "\n",
    "remote_dataset = client.RemoteDataset(\n",
    "    train_set, test_set, name=\"SMSSpamCollection\", privacy_limit=1_000_000.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scientist POV: Uploading the model and trigger training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Data Scientist side, we use the `list_remote_datasets` endpoint to get a list of available datasets on the server that we can use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SMSSpamCollection (5cd871636638c4cde41c672d735e2ab0af1628edc43ccd4bcdbc126c35e95fa1): size=64, desc=N/A']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Connection(\"localhost\").client.torch\n",
    "\n",
    "remote_datasets = client.list_remote_datasets()\n",
    "\n",
    "[str(ds) for ds in remote_datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the dataset uploaded previously is available as a `RemoteDataset ` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bastionlab.torch.learner.RemoteDataset at 0x7f39bcb487c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object is a pointer to the remote dataset uploaded previously. Note that this is a **pointer that contains only metadata and nothing else**. This way, the data scientist can play with remote datasets without users' data being exposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the `RemoteLearner` method to send the model to the server and to set all the necessary training parameters.\n",
    "\n",
    "To start training, we just call the `fit` method on the `RemoteLearner` object with an appropriate number of epochs and DP budget. \n",
    "Note that behind the scenes, a DP-SGD training loop will be used.\n",
    "\n",
    "We may finally retrieve a local copy of the trained model once the training is complete with the `get_model` method and test the model directly on the server with the `test` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending DistilBERT: 100%|████████████████████| 268M/268M [00:05<00:00, 47.3MB/s] \n",
      "Epoch 1/2 - train:  12%|██▌                 | 4/32 [00:18<02:07,  4.54s/batch, cross_entropy=10.0000 (+/- 3656.3914)]  "
     ]
    }
   ],
   "source": [
    "from bastionlab.torch.optimizer_config import Adam\n",
    "\n",
    "# The Data Scientist discovers available datasets and uses one of them to train their model\n",
    "client = Connection(\"localhost\").client.torch\n",
    "remote_learner = client.RemoteLearner(\n",
    "    model,\n",
    "    remote_datasets[0],\n",
    "    max_batch_size=2,\n",
    "    loss=\"cross_entropy\",\n",
    "    optimizer=Adam(lr=5e-5),\n",
    "    model_name=\"DistilBERT\",\n",
    ")\n",
    "\n",
    "remote_learner.fit(nb_epochs=2, eps=6.0)\n",
    "remote_learner.test(metric=\"accuracy\")\n",
    "\n",
    "trained_model = remote_learner.get_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "cfb725626286d8c8fc5334ffe77697f720dc23e64d3046271825a5556b528e7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
